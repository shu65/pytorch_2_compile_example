{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8MhTNeH5z4hak1n2PQi+9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shu65/pytorch_2_compile_example/blob/main/torch_2_0_compile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozM_wjkKShH0",
        "outputId": "a680579e-233d-48fe-8265-851f9fb9e0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Collecting torch\n",
            "  Downloading torch-2.0.0-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.14.1+cu116)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.15.1-cp39-cp39-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (0.13.1+cu116)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.0.1-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0\n",
            "  Downloading triton-2.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 KB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (63.4.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Collecting lit\n",
            "  Downloading lit-15.0.7.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 KB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=90003 sha256=41701ad97382399383e3d7d57a129599beab618af09461bad0e898acb9d79ddd\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/68/18/2ad49b416abb9139c8217c349fd9df0674da8f0d1952db2ea5\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.1+cu116\n",
            "    Uninstalling torchvision-0.14.1+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.1+cu116\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.13.1+cu116\n",
            "    Uninstalling torchaudio-0.13.1+cu116:\n",
            "      Successfully uninstalled torchaudio-0.13.1+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 2.0.0 which is incompatible.\n",
            "fastai 2.7.11 requires torch<1.14,>=1.7, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-15.0.7 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 torchaudio-2.0.1 torchvision-0.15.1 triton-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhInZ0RZSoP3",
        "outputId": "ead0b6c7-85dd-4b8c-a96d-a826d3cd4419"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch                         2.0.0\n",
            "torchaudio                    2.0.1\n",
            "torchsummary                  1.5.1\n",
            "torchtext                     0.14.1\n",
            "torchvision                   0.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPodUM5JUb_6",
        "outputId": "1faab85e-4600-4b12-ab3d-d8eb7a9e2c3e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Mar 17 18:26:12 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P0    28W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time \n",
        "\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch._dynamo\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "n_warmup_iters = 10\n",
        "n_iters = 500\n",
        "\n",
        "x = torch.randn(batch_size, 3, 224, 224).cuda()\n",
        "\n",
        "def get_mode():\n",
        "    return models.resnet18()"
      ],
      "metadata": {
        "id": "fUWz2XB4Suyh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_mode().cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "for i in range(n_warmup_iters):\n",
        "    optimizer.zero_grad()\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    out = model(x)\n",
        "    torch.cuda.synchronize()\n",
        "    forward_elapsed_time = time.time() - start\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    out.sum().backward()\n",
        "    backward_elapsed_time = time.time() - start\n",
        "    print(f\"default {i} iter forward: {forward_elapsed_time/1000:.3e} msec., backward: {backward_elapsed_time/1000:.3e} msec.\")\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"-\"*10)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "for i in range(n_iters):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(x)\n",
        "    out.sum().backward()\n",
        "    optimizer.step()\n",
        "torch.cuda.synchronize()\n",
        "elapsed_time = time.time() - start\n",
        "\n",
        "print(f\"default total:{elapsed_time:.3e} sec. {batch_size*n_iters/elapsed_time:.3e} imgs/sec.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wuK3l5oSzt3",
        "outputId": "13466d89-f291-47e9-bad2-4a934141bfc3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default 0 iter forward: 5.549e-03 msec., backward: 6.485e-04 msec.\n",
            "default 1 iter forward: 5.869e-05 msec., backward: 6.034e-06 msec.\n",
            "default 2 iter forward: 5.193e-05 msec., backward: 5.990e-06 msec.\n",
            "default 3 iter forward: 5.335e-05 msec., backward: 5.827e-06 msec.\n",
            "default 4 iter forward: 5.128e-05 msec., backward: 3.760e-06 msec.\n",
            "default 5 iter forward: 5.155e-05 msec., backward: 4.232e-06 msec.\n",
            "default 6 iter forward: 5.188e-05 msec., backward: 6.186e-06 msec.\n",
            "default 7 iter forward: 5.234e-05 msec., backward: 5.688e-06 msec.\n",
            "default 8 iter forward: 5.200e-05 msec., backward: 3.709e-06 msec.\n",
            "default 9 iter forward: 5.167e-05 msec., backward: 6.084e-06 msec.\n",
            "----------\n",
            "default total:7.868e+01 sec. 4.067e+02 imgs/sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch._dynamo.reset()\n",
        "\n",
        "model = get_mode().cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# compile\n",
        "compiled_model = torch.compile(model)\n",
        "for i in range(n_warmup_iters):\n",
        "    optimizer.zero_grad()\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    out = compiled_model(x)\n",
        "    torch.cuda.synchronize()\n",
        "    forward_elapsed_time = time.time() - start\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    out.sum().backward()\n",
        "    backward_elapsed_time = time.time() - start\n",
        "    print(f\"with compile {i} iter forward: {forward_elapsed_time/1000:.3e} msec., backward: {backward_elapsed_time/1000:.3e} msec.\")\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"-\"*10)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "for i in range(n_iters):\n",
        "    optimizer.zero_grad()\n",
        "    out = compiled_model(x)\n",
        "    out.sum().backward()\n",
        "    optimizer.step()\n",
        "torch.cuda.synchronize()\n",
        "elapsed_time = time.time() - start\n",
        "\n",
        "print(f\"with compile total:{elapsed_time:.3e} sec. {batch_size*n_iters/elapsed_time:.3e} imgs/sec.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLgaMHhrS248",
        "outputId": "476b094e-6bf0-4e0c-a4d2-5f984603c6fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "with compile 0 iter forward: 2.029e-02 msec., backward: 1.319e-02 msec.\n",
            "with compile 1 iter forward: 5.798e-05 msec., backward: 6.389e-06 msec.\n",
            "with compile 2 iter forward: 5.125e-05 msec., backward: 9.748e-06 msec.\n",
            "with compile 3 iter forward: 5.196e-05 msec., backward: 6.076e-06 msec.\n",
            "with compile 4 iter forward: 4.972e-05 msec., backward: 6.641e-06 msec.\n",
            "with compile 5 iter forward: 4.980e-05 msec., backward: 6.386e-06 msec.\n",
            "with compile 6 iter forward: 4.960e-05 msec., backward: 5.938e-06 msec.\n",
            "with compile 7 iter forward: 4.988e-05 msec., backward: 5.989e-06 msec.\n",
            "with compile 8 iter forward: 5.028e-05 msec., backward: 6.197e-06 msec.\n",
            "with compile 9 iter forward: 5.009e-05 msec., backward: 6.032e-06 msec.\n",
            "----------\n",
            "with compile total:7.337e+01 sec. 4.361e+02 imgs/sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch._dynamo.reset()\n",
        "\n",
        "model = get_mode().cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "# compile \n",
        "compiled_model = torch.compile(model, mode=\"reduce-overhead\")\n",
        "for i in range(n_warmup_iters):\n",
        "    optimizer.zero_grad()\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    out = compiled_model(x)\n",
        "    torch.cuda.synchronize()\n",
        "    forward_elapsed_time = time.time() - start\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    out.sum().backward()\n",
        "    backward_elapsed_time = time.time() - start\n",
        "    print(f\"with compile reduce-overhead {i} iter forward: {forward_elapsed_time/1000:.3e} msec., backward: {backward_elapsed_time/1000:.3e} msec.\")\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"-\"*10)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "for i in range(n_iters):\n",
        "    optimizer.zero_grad()\n",
        "    out = compiled_model(x)\n",
        "    out.sum().backward()\n",
        "    optimizer.step()\n",
        "torch.cuda.synchronize()\n",
        "elapsed_time = time.time() - start\n",
        "\n",
        "print(f\"with compile reduce-overhead total:{elapsed_time:.3e} sec. {batch_size*n_iters/elapsed_time:.3e} imgs/sec.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97qqRDJiS5yy",
        "outputId": "545f116c-5718-40e1-eed7-28bc361cde4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "with compile reduce-overhead 0 iter forward: 7.952e-03 msec., backward: 2.075e-03 msec.\n",
            "with compile reduce-overhead 1 iter forward: 5.474e-05 msec., backward: 3.117e-06 msec.\n",
            "with compile reduce-overhead 2 iter forward: 4.764e-05 msec., backward: 3.864e-06 msec.\n",
            "with compile reduce-overhead 3 iter forward: 4.787e-05 msec., backward: 2.983e-06 msec.\n",
            "with compile reduce-overhead 4 iter forward: 4.822e-05 msec., backward: 2.951e-06 msec.\n",
            "with compile reduce-overhead 5 iter forward: 4.855e-05 msec., backward: 3.038e-06 msec.\n",
            "with compile reduce-overhead 6 iter forward: 4.937e-05 msec., backward: 3.098e-06 msec.\n",
            "with compile reduce-overhead 7 iter forward: 4.825e-05 msec., backward: 2.992e-06 msec.\n",
            "with compile reduce-overhead 8 iter forward: 4.902e-05 msec., backward: 2.908e-06 msec.\n",
            "with compile reduce-overhead 9 iter forward: 4.765e-05 msec., backward: 3.263e-06 msec.\n",
            "----------\n",
            "with compile reduce-overhead total:7.752e+01 sec. 4.128e+02 imgs/sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch._dynamo.reset()\n",
        "\n",
        "model = get_mode().cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# compile\n",
        "compiled_model = torch.compile(model, mode=\"max-autotune\")\n",
        "for i in range(n_warmup_iters):\n",
        "    optimizer.zero_grad()\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    out = compiled_model(x)\n",
        "    torch.cuda.synchronize()\n",
        "    forward_elapsed_time = time.time() - start\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    out.sum().backward()\n",
        "    backward_elapsed_time = time.time() - start\n",
        "    print(f\"with compile max-autotune {i} iter forward: {forward_elapsed_time/1000:.3e} msec., backward: {backward_elapsed_time/1000:.3e} msec.\")\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"-\"*10)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "for i in range(n_iters):\n",
        "    optimizer.zero_grad()\n",
        "    out = compiled_model(x)\n",
        "    out.sum().backward()\n",
        "    optimizer.step()\n",
        "torch.cuda.synchronize()\n",
        "elapsed_time = time.time() - start\n",
        "\n",
        "print(f\"with compile max-autotune total:{elapsed_time:.3e} sec. {batch_size*n_iters/elapsed_time:.3e} imgs/sec.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umVCY1n7S8Tq",
        "outputId": "8d2f1efe-8f2d-421c-8902-2ff4f5a74989"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2023-03-17 18:31:06,314] torch._inductor.utils: [WARNING] not enough cuda cores to use max_autotune mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "with compile max-autotune 0 iter forward: 6.270e-03 msec., backward: 1.910e-03 msec.\n",
            "with compile max-autotune 1 iter forward: 5.046e-05 msec., backward: 3.277e-06 msec.\n",
            "with compile max-autotune 2 iter forward: 5.078e-05 msec., backward: 3.359e-06 msec.\n",
            "with compile max-autotune 3 iter forward: 5.099e-05 msec., backward: 3.483e-06 msec.\n",
            "with compile max-autotune 4 iter forward: 5.013e-05 msec., backward: 5.197e-06 msec.\n",
            "with compile max-autotune 5 iter forward: 5.044e-05 msec., backward: 3.144e-06 msec.\n",
            "with compile max-autotune 6 iter forward: 5.031e-05 msec., backward: 3.129e-06 msec.\n",
            "with compile max-autotune 7 iter forward: 5.008e-05 msec., backward: 3.624e-06 msec.\n",
            "with compile max-autotune 8 iter forward: 5.063e-05 msec., backward: 3.101e-06 msec.\n",
            "with compile max-autotune 9 iter forward: 5.024e-05 msec., backward: 3.330e-06 msec.\n",
            "----------\n",
            "with compile max-autotune total:7.335e+01 sec. 4.362e+02 imgs/sec.\n"
          ]
        }
      ]
    }
  ]
}